---
title: "Mistral Hackathon in London"
date: "2024-10-20"
description: "Hackathon Experience report"
author:
  name: "Max Neuwinger"
  image: "/images/avatar.webp"
tags: ["Mistral", "LLM"]
featured: false
category: "projects"
---

Sometimes the best adventures start as a complete logistical disaster. That’s exactly what happened with my trip to the Mistral AI Hackathon in London—a chaotic mix of expired passports, spontaneous flight bookings, and ending up in a hacker house with people I met on the internet.

<img
  src="/images/Londonhackathon/stage.webp"
  alt="BStage"
  style={{ width: "50%" }}
  className="mx-auto"
/>

## The Pre-Hackathon Chaos

The whole thing kicked off when my Evolonic teammate found out about the Mistral AI hackathon in London just a week before the event. We applied on a whim, both got accepted, and were super hyped. But then reality hit: thanks to post-Brexit travel rules, you need a valid passport to enter the UK. My teammate's passport was expired. Trip canceled. I was honestly pretty bummed out.

But fast forward to Wednesday afternoon. I was hanging out and studying with some friends at ETH, and a friend casually mentioned she had also gotten into the London hackathon but wasn't going to go because her team fell through. I immediately pitched the idea of teaming up. One thing led to another, and within an hour, we were booking flights to London for that Friday.

## The Hacker House Gamble

Flights were sorted, but we had nowhere to sleep. While frantically searching online late at night, I saw that a startup called Chipp AI was hosting a hacker house for the event. The deadline was literally midnight, so we speed-ran the application.

Honestly, I fully expected to get ghosted, but we got a direct reply from the CEO of Chipp AI asking if we were actually serious about flying in on such short notice. We confirmed, but the whole flight to London we were kind of wondering if this place actually existed or if we were about to be stranded in the city.

Luckily, we landed Friday night, made our way to the house, and the Chipp AI team was incredibly welcoming. It was a super nice setup, and it felt great to just crash on a bed before the actual coding marathon started.

## The Hackathon & The Idea

Saturday morning, we headed over to the a16z offices where the event was hosted. The venue was incredible—modern, packed with energy, and loaded with top-tier food and drinks to keep everyone fueled. We originally started as a trio, but one person split off to do their own thing, leaving just my friend and me to tackle a project together.

We were tossing around a few ideas, but a chat with Nick Carmont from Mistral AI really steered us in the right direction. We decided to tackle a problem that drives everyone crazy: AI-generated presentations usually look terrible.

Why? Because traditional LLMs are blind. They generate text for a slide, but they have no idea if that text is overflowing, if the formatting is completely broken, or if the images are placed in a way that makes sense.

Our fix was to use Mistral's brand new multimodal model, Pixtral (their first model that can actually process images), to create a visual feedback loop.

## How We Built It

We spent the first day mapping things out and building the core, eventually passing out and coming back fresh on Sunday to sprint to the 2 PM finish line.

Here is what our pipeline actually looked like:

<img
  src="/images/Londonhackathon/diagram.webp"
  alt="BStage"
  style={{ width: "50%" }}
  className="mx-auto"
/>

1. First, we feed the system a scientific paper as a PDF.
2. We run a custom preprocessing script to rip out the text, the mathematical formulas, and the actual figures/images.
3. We send that parsed data to GPT-4o and ask it to draft a LaTeX Beamer presentation, plugging in the extracted images where they belong.
4. We compile the LaTeX code into a PDF and automatically take screenshots of every single generated slide.
5. Here is where the magic happens: we feed those slide screenshots into Mistral's Pixtral model. It looks at the slide, realizes "hey, this text is overlapping the image" or "this slide is way too crowded," and generates a specific critique.
6. We pass that critique back to GPT-4o to fix the LaTeX code, recompile, and loop the process until the slide actually looks good.

## The Reality Check (A.K.A. The Bugs)

Of course, the diagram makes it look way easier than it actually was. We hit some massive technical walls along the way.

Parsing PDFs is universally awful, but extracting images dynamically and keeping them linked to the right context was especially painful. Then we had to deal with GPT-4o's context window limits; if we stuffed too much of the paper in at once, it would lose the plot or just hallucinate where images should go.

But the real boss fight was LaTeX compilation. If GPT-4o messed up a single bracket, the entire pipeline crashed. My teammate was trying to run the compilation environment on MacOS and was just running into endless errors, whereas my Linux setup ended up being a lot more stable.

On top of that, Mistral’s new vision API was acting super erratic. We were getting random failures that had nothing to do with rate limits. We had to write some pretty aggressive retry and backoff logic just to keep the pipeline from dying halfway through a presentation generation.

## The Results

Despite the headaches, the feedback loop actually worked! Having an AI visually roast its own slides to make them better produced some genuinely cool results.

### Example 1: Fixing the Clutter

**Initial Generation:**
<img
  src="/images/Londonhackathon/initial1.webp"
  alt="Initial slide showing first generation attempt"
  style={{ width: "50%" }}
  className="mx-auto"
/>

**After Two Refinement Iterations:**
<img
  src="/images/Londonhackathon/after1.webp"
  alt="Improved slide after feedback loop"
  style={{ width: "50%" }}
  className="mx-auto"
/>

You can clearly see how Pixtral caught the awful layout in the first try and forced GPT-4o to clean up the hierarchy and make it actually readable.

### Example 2: Balancing Visuals

**Initial Generation:**
<img
  src="/images/Londonhackathon/initial2.webp"
  alt="Second example of initial slide generation"
  style={{ width: "50%" }}
  className="mx-auto"
/>

**After Two Refinement Iterations:**
<img
  src="/images/Londonhackathon/after2.webp"
  alt="Second example of improved slide"
  style={{ width: "50%" }}
  className="mx-auto"
/>

Here, the system learned to balance the text density with the embedded figure much better after a couple of passes.

## Wrapping Up

When the deadline hit, we got to present our pipeline to the judges. The technical discussions were super interesting, and they asked a lot of great questions about how we managed the visual feedback loop. We didn't end up taking home a prize, but honestly, just building something this complex over a weekend and having it actually run was a huge win.

A massive shoutout to a16z for hosting an incredibly smooth event, the Mistral team for letting us play with their new tech, and especially Scott Hunter and the whole Chipp AI crew for the hacker house. I’ve actually kept using Chipp's tools since the hackathon—they are building a genuinely great community over there (you can check them out at [chipp](https://www.chipp.ai/)).

This whole weekend was a great reminder of why I love these events. Sometimes the best thing you can do on a random Wednesday afternoon at ETH is just book a flight and figure the rest out later.
